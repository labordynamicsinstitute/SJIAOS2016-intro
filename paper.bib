% This file was created with JabRef 2.11.1.
% Encoding: ISO8859_1


@InCollection{AbowdEtAl2009,
  Title                    = {The {LEHD} Infrastructure Files and the Creation of the {Q}uarterly {W}orkforce {I}ndicators},
  Author                   = {John M. Abowd and Bryce E. Stephens and Lars Vilhuber and Fredrik Andersson and Kevin L. McKinney and Marc Roemer and Simon D. Woodcock},
  Booktitle                = {Timothy Dunne, J. Bradford Jensen, and Mark J. Roberts},
  Publisher                = {University of Chicago Press},
  Year                     = {2009},
  Crossref                 = {DunneJensenRoberts2009},
  File                     = {AbowdStephensVilhuber2005-LEHD-final.pdf:L/LEHD/AbowdStephensVilhuber2005-LEHD-final.pdf:PDF},
  Owner                    = {vilhuber},
  Timestamp                = {2007.03.13}
}

@TechReport{AbowdEtAl2012,
  Title                    = {Dynamically consistent noise infusion and partially synthetic data as confidentiality protection measures for related time-series},
  Author                   = {John M. Abowd and Kaj Gittings and Kevin L. McKinney and Bryce E. Stephens and Lars Vilhuber and Simon Woodcock},
  Institution              = {Federal Committee on Statistical Methodology},
  Year                     = {2012},
  Month                    = {January},
  Owner                    = {vilhuber},
  Timestamp                = {2012.05.21},
  Url                      = {http://www.fcsm.gov/events/papers2012.html}
}

@Article{AbowdSchmutte_BPEA2015,
  Title                    = {Economic analysis and statistical disclosure limitation},
  Author                   = {John M. Abowd and Ian Schmutte},
  Journal                  = {Brookings Papers on Economic Activity},
  Year                     = {2015},
  Volume                   = {Fall 2015},
  Abstract                 = {This paper explores the consequences for economic research of methods used by statistical agencies to protect confidentiality of their respondents. We first review the concepts of statistical disclosure limitation for an audience of economists who may be unfamiliar with these methods. Our main objective is to shed light on the effects of statistical disclosure limitation for empirical economic research. In general, the standard approach of ignoring statistical disclosure limitation leads to incorrect inference. We formalize statistical disclosure methods in a model of the data publication process. In the model, the statistical agency collects data from a population, but published a version of the data that have been intentionally distorted. The model allows us to characterize what it means for statistical disclosure limitation to be ignorable, and to characterize what happens when it is not. We then consider the effects of statistical disclosure limitation for regression analysis, instrumental variable analysis, and regression discontinuity design. Because statistical agencies do not always report the methods they use to protect confidentiality, we use our model to characterize settings in which statistical disclosure limitation methods are discoverable; that is, they can be learned from the released data. We conclude with advice for researchers, journal editors, and statistical agencies.},
  Copyright                = {Copyright © 2015 Brookings Institution Press},
  ISSN                     = {00072303},
  Jstor_articletype        = {research-article},
  Language                 = {English},
  Publisher                = {Brookings Institution Press},
  Url                      = {http://www.brookings.edu/about/projects/bpea/papers/2015/economic-analysis-statistical-disclosure-limitation}
}

@TechReport{ssafinal,
  Title                    = {Final Report to the {Social Security Administration} on the {SIPP/SSA/IRS}
{Public} {Use} {File} {Project}},
  Author                   = {John M. Abowd and Martha Stinson and Gary Benedetto},
  Institution              = {U.S. Census Bureau},
  Year                     = {2006},
  Owner                    = {vilhuber},
  Timestamp                = {2013.10.07},
  Url                      = {http://www2.vrdc.cornell.edu/news/?p=308}
}

@Electronic{AbowdVilhuber2010,
  Title                    = {Synthetic Data Server},
  Author                   = {Abowd, John M. and Lars Vilhuber},
  Url                      = {http://www.vrdc.cornell.edu/sds/},
  Year                     = {2010},
  Owner                    = {vilhuber},
  Timestamp                = {2012.05.18}
}

@InProceedings{Bender2009,
  Title                    = {The {RDC} of the {Federal Employment Agency} as a part of the {German} {RDC} Movement},
  Author                   = {Stefan Bender},
  Booktitle                = {Comparative Analysis of Enterprise Data, 2009 Conference},
  Year                     = {2009},
  Eventtitle               = {Comparative Analysis of Enterprise Data, 2009 Conference},
  Url                      = {http://gcoe.ier.hit-u.ac.jp/CAED/index.html},
  Urldate                  = {2014-05-05},
  Venue                    = {Tokyo}
}

@Article{BenedettoEtAl2007,
  Title                    = {Using Worker Flows in the Analysis of the Firm},
  Author                   = {Gary Benedetto and John Haltiwanger and Julia Lane and Kevin McKinney},
  Journal                  = jbes,
  Year                     = {2007},
  Month                    = jul,
  Number                   = {3},
  Pages                    = {299-313},
  Volume                   = {25},
  Comment                  = {see also LEHD TP-2003-09},
  File                     = {tp-2003-09.pdf:L/LEHD/tp-2003-09.pdf:PDF},
  Owner                    = {vilhuber},
  Timestamp                = {2007.01.12}
}

@Article{BradleyEtAl2014,
  Title                    = {{Mixed Effects Modeling for Areal Data that Exhibit Multivariate-Spatio-Temporal 
 Dependencies}},
  Author                   = {{Bradley}, J.~R. and {Holan}, S.~H. and {Wikle}, C.~K.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2014},
  Month                    = jul,
  Adsnote                  = {Provided by the SAO/NASA Astrophysics Data System},
  Adsurl                   = {http://adsabs.harvard.edu/abs/2014arXiv1407.7479B},
  Archiveprefix            = {arXiv},
  Eprint                   = {1407.7479},
  Keywords                 = {Statistics - Methodology},
  Primaryclass             = {stat.ME}
}

@TechReport{BHP,
  Title                    = {Establishment {H}istory {P}anel ({BHP}) },
  Author                   = {{Bundesagentur f\"ur Arbeit}},
  Institution              = {Research Data Centre (FDZ) of
 the German Federal Employment Agency (BA) at the Institute for
 Employment Research (IAB) [distributor]},
  Year                     = {2013},
  Address                  = {N\"urnberg, Germany},
  Type                     = {[Computer file]},
  HowPublished             = {Computer file},
  Organization             = {Research Data Centre (FDZ) of German Federal Employment Agency (BA) at the Institute for
 Employment Research (IAB) [distributor]},
  Owner                    = {vilhuber},
  Timestamp                = {2013.06.10}
}

@TechReport{IABEstabPanel,
  Title                    = {{IAB} {E}stablishment {P}anel},
  Author                   = {{Bundesagentur f\"ur Arbeit}},
  Institution              = {Research Data Centre (FDZ) of
 the German Federal Employment Agency (BA) at the Institute for
 Employment Research (IAB) [distributor]},
  Year                     = {2013},
  Address                  = {N\"urnberg, Germany},
  Type                     = {[Computer file]},
  HowPublished             = {Computer file},
  Organization             = {Research Data Centre (FDZ) of German Federal Employment Agency (BA) at the Institute for
 Employment Research (IAB) [distributor]},
  Owner                    = {vilhuber},
  Timestamp                = {2013.06.10}
}

@TechReport{JVS,
  Title                    = {German {J}ob {V}acancy {S}urvey of the {IAB}},
  Author                   = {{Bundesagentur f\"ur Arbeit}},
  Institution              = {Research Data Centre (FDZ) of
 the German Federal Employment Agency (BA) at the Institute for
 Employment Research (IAB) [distributor]},
  Year                     = {2013},
  Address                  = {N\"urnberg, Germany},
  Type                     = {[Computer file]},
  HowPublished             = {Computer file},
  Organization             = {Research Data Centre (FDZ) of German Federal Employment Agency (BA) at the Institute for
 Employment Research (IAB) [distributor]},
  Owner                    = {vilhuber},
  Timestamp                = {2013.06.10}
}

@Article{10.1257/jep.28.3.3,
  Title                    = {The Role of Entrepreneurship in US Job Creation and Economic Dynamism},
  Author                   = {Decker, Ryan and Haltiwanger, John and Jarmin, Ron and Miranda, Javier},
  Journal                  = {Journal of Economic Perspectives},
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {3-24},
  Volume                   = {28},
  Abstract                 = {An optimal pace of business dynamics—encompassing the processes of entry, exit, expansion, and contraction—would balance the benefits of productivity and economic growth against the costs to firms and workers associated with reallocation of productive resources. It is difficult to prescribe what the optimal pace should be, but evidence accumulating from multiple datasets and methodologies suggests that the rate of business startups and the pace of employment dynamism in the US economy has fallen over recent decades and that this downward trend accelerated after 2000. A critical factor in accounting for the decline in business dynamics is a lower rate of business startups and the related decreasing role of dynamic young businesses in the economy. For example, the share of US employment accounted for by young firms has declined by almost 30 percent over the last 30 years. These trends suggest that incentives for entrepreneurs to start new firms in the United States have diminished over time. We do not identify all the factors underlying these trends in this paper but offer some clues based on the empirical patterns for specific sectors and geographic regions.},
  Doi                      = {10.1257/jep.28.3.3},
  Url                      = {http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.3.3}
}

@TechReport{RePEc:iab:iabdpa:201006,
  Title                    = {Multiple imputation of missing values in the wave 2007 of the IAB Establishment Panel},
  Author                   = {Drechsler,J\"org},
  Institution              = {Institut f\"ur Arbeitsmarkt- und Berufsforschung (IAB), N\"urnberg [Institute for Employment Research, Nuremberg, Germany]},
  Year                     = {2010},
  Month                    = Feb,
  Number                   = {201006},
  Type                     = {IAB Discussion Paper},
  Abstract                 = {\&quot;The basic concept of multiple imputation is straightforward and easy to understand, but the application to real data imposes many implementation problems. To define useful imputation models for a dataset that consists of categorical and of continuous variables with distributions that are anything but normal, contains skip patterns and all sorts of logical constraints is a challenging task. In this paper, we review different approaches to handle these problems and illustrate their successful implementation for a complex imputation project at the German Institute for Employment Research (IAB): The imputation of missing values in one wave of the IAB Establishment Panel.\&quot; (Author's abstract, IAB-Doku) ((en))},
  Keywords                 = {Missing Data-Technik; Datenqualität; IAB-Betriebspanel; statistische Methode; Imputationsverfahren},
  Url                      = {http://ideas.repec.org/p/iab/iabdpa/201006.html}
}

@Book{dre:2011,
  Title                    = {Synthetic Datasets for Statistical Disclosure Control--Theory and Implementation},
  Author                   = {Drechsler, J.},
  Publisher                = {New York: Springer},
  Year                     = {2011}
}

@TechReport{RePEc:iab:iabfme:201101_de,
  Title                    = {Synthetische {S}cientific-Use-Files der {W}elle 2007 des {IAB}-{B}etriebspanels},
  Author                   = {Drechsler, J\"org},
  Institution              = {Institute for Employment Research, Nuremberg, Germany},
  Year                     = {2011},
  Month                    = Jan,
  Number                   = {201101\_de},
  Type                     = {FDZ Methodenreport},
  Abstract                 = {\&quot;Providing scientific use files for business surveys is a difficult task. Due to smaller populations, higher sampling rates, and skewed distributions disclosure risks are much higher than for household surveys. Simple measures like coarsening are not sufficient to protect the data. The aim of generating synthetic datasets is to release data that provide a high level of data utility while guaranteeing the confidentiality of the survey respondent. To achieve this, sensitive variables and variables that could be used for re-identification purposes are replaced with multiple imputations. This report gives a short introduction to the topic and discusses some aspects that analysts should keep in mind when using the synthetic datasets. Furthermore, the report describes how valid inferences can be obtained based on the synthetic datasets and provides some first data utility evaluations that indicate the potentials but also the limits of the generated datasets.\&quot; (Author's abstract, IAB-Doku) ((en))},
  Keywords                 = {IAB-Betriebspanel; Imputationsverfahren; Datenanonymisierung; Datenschutz; Betriebsdatenerfassung; N},
  Url                      = {http://ideas.repec.org/p/iab/iabfme/201101_de.html}
}

@Article{RePEc:spr:alstar:v:95:y:2011:i:1:p:1-26,
  Title                    = {Multiple imputation in practice: a case study using a complex German establishment survey},
  Author                   = {J\"org Drechsler},
  Journal                  = {AStA Advances in Statistical Analysis},
  Year                     = {2011},
  Month                    = {March},
  Number                   = {1},
  Pages                    = {1-26},
  Volume                   = {95},
  Abstract                 = {No abstract is available for this item.},
  Keywords                 = {Multiple imputation; Fully conditional specification; Model evaluation; Imputation software; IAB Est},
  Url                      = {http://ideas.repec.org/a/spr/alstar/v95y2011i1p1-26.html}
}

@Article{RePEc:taf:japsta:v:39:y:2012:i:2:p:243-265,
  Title                    = {New data dissemination approaches in old {E}urope -- synthetic datasets for a {G}erman establishment survey},
  Author                   = {J\"org Drechsler},
  Journal                  = {Journal of Applied Statistics},
  Year                     = {2012},
  Month                    = {April},
  Number                   = {2},
  Pages                    = {243-265},
  Volume                   = {39},
  Abstract                 = { Disseminating microdata to the public that provide a high level of data utility, while at the same time guaranteeing the confidentiality of the survey respondent is a difficult task. Generating multiply imputed synthetic datasets is an innovative statistical disclosure limitation technique with the potential of enabling the data disseminating agency to achieve this twofold goal. So far, the approach was successfully implemented only for a limited number of datasets in the U.S. In this paper, we present the first successful implementation outside the U.S.: the generation of partially synthetic datasets for an establishment panel survey at the German Institute for Employment Research. We describe the whole evolution of the project: from the early discussions concerning variables at risk to the final synthesis. We also present our disclosure risk evaluations and provide some first results on the data utility of the generated datasets. A variance-inflated imputation model is introduced that incorporates additional variability in the model for records that are not sufficiently protected by the standard synthesis.},
  Url                      = {http://ideas.repec.org/a/taf/japsta/v39y2012i2p243-265.html}
}

@TechReport{RePEc:iab:iabdpa:200711,
  Title                    = {A new approach for disclosure control in the IAB Establishment Panel : multiple imputation for a better data access},
  Author                   = {Drechsler, J\"org and Dundler, Agnes and Bender, Stefan and R\"assler, Susanne and Zwick, Thomas},
  Institution              = {Institut f\"ur Arbeitsmarkt- und Berufsforschung (IAB), N\"urnberg [Institute for Employment Research, Nuremberg, Germany]},
  Year                     = {2007},
  Month                    = Feb,
  Number                   = {200711},
  Type                     = {IAB Discussion Paper},
  Abstract                 = {\&quot;For micro-datasets considered for release as scientific or public use files, statistical agencies have to face the dilemma of guaranteeing the confidentiality of survey respondents on the one hand and offering sufficiently detailed data on the other hand. For that reason a variety of methods to guarantee disclosure control is discussed in the literature. In this paper, we present an application of Rubin's (1993) idea to generate synthetic datasets from existing confidential survey data for public release. We use a set of variables from the 1997 wave of the German IAB Establishment Panel and evaluate the quality of the approach by comparing results from an analysis by Zwick (2005) with the original data with the results we achieve for the same analysis run on the dataset after the imputation procedure. The comparison shows that valid inferences can be obtained using the synthetic datasets in this context, while confidentiality is guaranteed for the survey participants.\&quot; (Author's abstract, IAB-Doku) ((en))},
  Keywords                 = {IAB-Betriebspanel; Datensicherheit; Datenschutz; Datenaufbereitung; Datenanonymisierung; Imputations},
  Url                      = {http://ideas.repec.org/p/iab/iabdpa/200711.html}
}

@Article{DrechslerReiter2009,
  Title                    = {Disclosure Risk and Data Utility for Partially Synthetic Data: An Empirical Study Using the {G}erman {IAB} {E}stablishment {S}urvey. , Vol. 25, 589-603.},
  Author                   = {Drechsler, J\"org and Reiter, Jerome P.},
  Journal                  = {Journal of Official Statistics},
  Year                     = {2009},
  Month                    = {December},
  Number                   = {12},
  Pages                    = {589-603},
  Volume                   = {25},
  Abstract                 = {When intense redaction is needed to protect the confidentiality of data subjects' identities and sensitive attributes, statistical agencies can use synthetic data approaches. To create synthetic data, the agency replaces identifying or sensitive values with draws from statistical models estimated from the confidential data. Many agencies are reluctant to implement this idea because (i) the quality of the generated data depends strongly on the quality of the underlying models, and (ii) developing effective synthesis models can be a labor-intensive and difficult task. Recently, there have been suggestions that agencies use nonparametric methods from the machine learning literature to generate synthetic data. These methods can estimate non-linear relationships that might otherwise be missed and can be run with minimal tuning, thus considerably reducing burdens on the agency. Four synthesizers based on machine learning algorithms-classification and regression trees, bagging, random forests, and support vector machines-are evaluated in terms of their potential to preserve analytical validity while reducing disclosure risks. The evaluation is based on a repeated sampling simulation with a subset of the 2002 Uganda census public use sample data. The simulation suggests that synthesizers based on regression trees can result in synthetic datasets that provide reliable estimates and low disclosure risks, and that these synthesizers can be implemented easily by statistical agencies.},
  Keywords                 = { Census Confidentiality Disclosure Imputation Microdata Synthetic},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12},
  Url                      = {http://ideas.repec.org/a/eee/csdana/v55y2011i12p3232-3243.html}
}

@Article{RePEc:bes:jnlasa:v:105:i:492:y:2010:p:1347-1357,
  Title                    = {Sampling With Synthesis: A New Approach for Releasing Public Use Census Microdata},
  Author                   = {Drechsler, J\"org and Reiter, Jerome P.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2010},
  Number                   = {492},
  Pages                    = {1347-1357},
  Volume                   = {105},
  Abstract                 = {No abstract is available for this item.},
  Url                      = {http://ideas.repec.org/a/bes/jnlasa/v105i492y2010p1347-1357.html}
}

@Article{RePEc:eee:csdana:v:55:y:2011:i:12:p:3232-3243,
  Title                    = {An empirical evaluation of easily implemented, nonparametric methods for generating synthetic datasets},
  Author                   = {Drechsler, J\"org and Reiter, Jerome P.},
  Journal                  = {Computational Statistics \& Data Analysis},
  Year                     = {2011},
  Month                    = {December},
  Number                   = {12},
  Pages                    = {3232-3243},
  Volume                   = {55},
  Abstract                 = {When intense redaction is needed to protect the confidentiality of data subjects' identities and sensitive attributes, statistical agencies can use synthetic data approaches. To create synthetic data, the agency replaces identifying or sensitive values with draws from statistical models estimated from the confidential data. Many agencies are reluctant to implement this idea because (i) the quality of the generated data depends strongly on the quality of the underlying models, and (ii) developing effective synthesis models can be a labor-intensive and difficult task. Recently, there have been suggestions that agencies use nonparametric methods from the machine learning literature to generate synthetic data. These methods can estimate non-linear relationships that might otherwise be missed and can be run with minimal tuning, thus considerably reducing burdens on the agency. Four synthesizers based on machine learning algorithms-classification and regression trees, bagging, random forests, and support vector machines-are evaluated in terms of their potential to preserve analytical validity while reducing disclosure risks. The evaluation is based on a repeated sampling simulation with a subset of the 2002 Uganda census public use sample data. The simulation suggests that synthesizers based on regression trees can result in synthetic datasets that provide reliable estimates and low disclosure risks, and that these synthesizers can be implemented easily by statistical agencies.},
  Keywords                 = { Census Confidentiality Disclosure Imputation Microdata Synthetic},
  Url                      = {http://ideas.repec.org/a/eee/csdana/v55y2011i12p3232-3243.html}
}

@Article{DrechslerReiter2012,
  Title                    = {Combining synthetic data with subsampling to create public use microdata files for large scale surveys},
  Author                   = {Drechsler, J. and Reiter, J. P.},
  Year                     = {2012},
  Month                    = {June},
  Number                   = {1},
  Pages                    = {73-79},
  Volume                   = {38},
  Journaltitle             = {Survey Methodology}
}

@TechReport{DrechslerVilhuber2013,
  Title                    = {Replicating the {S}ynthetic {LBD} with {G}erman Establishment Data},
  Author                   = {Drechsler, J\"org and Vilhuber, Lars},
  Institution              = {World Statistics Conference},
  Year                     = {2013},
  Type                     = {Presentation},
  Owner                    = {vilhuber},
  Timestamp                = {2013.06.16}
}

@TechReport{RePEc:cen:wpaper:14-13,
  Title                    = {{A First Step Towards A German {SynLBD}: Constructing A {German} Longitudinal Business Database}},
  Author                   = {Jorg Drechsler and Lars Vilhuber},
  Institution              = {Center for Economic Studies, U.S. Census Bureau},
  Year                     = {2014},
  Month                    = Feb,
  Number                   = {14--13},
  Type                     = {Working Papers},
  Abstract                 = {One major criticism against the use of synthetic data has been that the efforts necessary to generate useful synthetic data are so in- tense that many statistical agencies cannot afford them. We argue many lessons in this evolving field have been learned in the early years of synthetic data generation, and can be used in the development of new synthetic data products, considerably reducing the required in- vestments. The final goal of the project described in this paper will be to evaluate whether synthetic data algorithms developed in the U.S. to generate a synthetic version of the Longitudinal Business Database (LBD) can easily be transferred to generate a similar data product for other countries. We construct a German data product with infor- mation comparable to the LBD - the German Longitudinal Business Database (GLBD) - that is generated from different administrative sources at the Institute for Employment Research, Germany. In a fu- ture step, the algorithms developed for the synthesis of the LBD will be applied to the GLBD. Extensive evaluations will illustrate whether the algorithms provide useful synthetic data without further adjustment. The ultimate goal of the project is to provide access to multiple synthetic datasets similar to the SynLBD at Cornell to enable comparative studies between countries. The Synthetic GLBD is a first step towards that goal.},
  Keywords                 = {confidentiality; comparative studies; German Longitudinal Business Database; synthetic data},
  Url                      = {http://ideas.repec.org/p/cen/wpaper/14-13.html}
}

@Article{SJIAOSDV2014,
  Title                    = {{A First Step Towards A German {SynLBD}: Constructing A {German} Longitudinal Business Database}},
  Author                   = {Jorg Drechsler and Lars Vilhuber},
  Journal                  = {Statistical Journal of the IAOS},
  Year                     = {2014},
  Month                    = May,
  Number                   = {2},
  Pages                    = {137--142},
  Volume                   = {30},
  Abstract                 = {One major criticism against the use of synthetic data has been that the efforts necessary to generate useful synthetic data are so in- tense that many statistical agencies cannot afford them. We argue many lessons in this evolving field have been learned in the early years of synthetic data generation, and can be used in the development of new synthetic data products, considerably reducing the required in- vestments. The final goal of the project described in this paper will be to evaluate whether synthetic data algorithms developed in the U.S. to generate a synthetic version of the Longitudinal Business Database (LBD) can easily be transferred to generate a similar data product for other countries. We construct a German data product with infor- mation comparable to the LBD - the German Longitudinal Business Database (GLBD) - that is generated from different administrative sources at the Institute for Employment Research, Germany. In a fu- ture step, the algorithms developed for the synthesis of the LBD will be applied to the GLBD. Extensive evaluations will illustrate whether the algorithms provide useful synthetic data without further adjustment. The ultimate goal of the project is to provide access to multiple synthetic datasets similar to the SynLBD at Cornell to enable comparative studies between countries. The Synthetic GLBD is a first step towards that goal.},
  Keywords                 = {confidentiality; comparative studies; German Longitudinal Business Database; synthetic data}
}

@Article{EvansZayatzSlanta1998,
  Title                    = {Using Noise for Disclosure Limitation of Establishment Tabular Data},
  Author                   = {Evans, Timothy and Laura Zayatz and John Slanta},
  Journal                  = {Journal of Official Statistics},
  Year                     = {1998},
  Month                    = {december}
}

@TechReport{Fort2013,
  Title                    = {Applying a Consistent Industry Classification Across Time},
  Author                   = {Teresa Fort},
  Institution              = {Center for Economic Studies},
  Year                     = {2013},
  Owner                    = {fort}
}

@PhdThesis{Gittings2009thesis,
  Title                    = {Essays in labor economics and synthetic data methods},
  Author                   = {Robert Kaj Gittings},
  School                   = {Cornell University},
  Year                     = {2009},
  Type                     = {Ph.D.},
  Abstract                 = {Three topics are investigated in these chapters: the causes and consequences
 of lateral job mobility within firms, the impact of incentives on
 human behavior in the context of capital punishment and deterrence,
 and the development of new synthetic data methods for confidentiality
 protection of public use data. The extent and importance of lateral
 mobility is not well-established in economics and Chapter 1 contributes
 new and important findings to the literature. Using a panel of more
 than 500 firms and 48,000 white-collar workers, I find relatively
 high rates of lateral mobility, that this mobility is statistically
 different from other transitions, and that the compensation growth
 associated with lateral mobility is economically meaningful. I also
 investigate the relationships between worker performance, compensation
 growth and job mobility. Even when controlling for productivity differences,
 significant earnings growth occurs directly through the change in
 jobs. The results provide some evidence that the observed lateral
 mobility may be the result of job rotation. In light of continued
 debate of whether capital punishment deters crime, Chapter 2 revisits
 my previous work on this issue and shows that the deterrence results
 hold under alternative measurements of key variables, multiple statistical
 specifications and subsets of the data. Chapter 3 develops methodology
 that solves the need for statistical agencies to suppress certain
 data items because releasing those cells to the public yields a risk
 of exposing someone's personal information. I show that the synthetic
 data adequately protect the confidential data and are superior in
 terms of its analytical validity.},
  Owner                    = {vilhuber},
  Timestamp                = {2012.04.15}
}

@Other{BDS2,
  Title                    = {Jobs Created from Business Startups in the {U}nited {S}tates},
  Author                   = {John Haltiwanger and Ron Jarmin and Javier Miranda},
  Institution              = {Ewing Marion Kauffman Foundation},
  Number                   = {1},
  Type                     = {BDS Brief},
  Url                      = {https://www.census.gov/ces/pdf/BDS_StatBrief1_Jobs_Created.pdf},
  Urldate                  = {2014-05-08},
  Year                     = {2008}
}

@TechReport{NBERw16300,
  Title                    = {Who Creates Jobs? {S}mall vs. Large vs. Young},
  Author                   = {John C. Haltiwanger and Ron S. Jarmin and Javier Miranda},
  Institution              = {National Bureau of Economic Research},
  Year                     = {2010},
  Month                    = {August},
  Number                   = {16300},
  Type                     = {Working Paper},
  Abstract                 = {The view that small businesses create the most jobs remains appealing to policymakers and small business advocates. Using data from the Census Bureau Business Dynamics Statistics and Longitudinal Business Database, we explore the many issues at the core of this ongoing debate. We find that the relationship between firm size and employment growth is sensitive to these issues. However, our main finding is that once we control for firm age there is no systematic relationship between firm size and growth. Our findings highlight the important role of business startups and young businesses in U.S. job creation.},
  Series                   = {Working Paper Series},
  Url                      = {http://www.nber.org/papers/w16300}
}

@InProceedings{hawala2008,
  Title                    = {Producing partially synthetic data to avoid disclosure},
  Author                   = {Hawala, Sam},
  Booktitle                = {Proceedings of the Joint Statistical Meetings},
  Year                     = {2008},
  Organization             = {American Statistical Association},
  Location                 = {Alexandria, VA},
  Url                      = {http://www.amstat.org/sections/srms/proceedings/y2008/Files/301018.pdf},
  Urldate                  = {2015-07-15}
}

@TechReport{RePEc:iab:iabfme:201006_en,
  Title                    = {Using worker flows in the analysis of establishment turnover: {E}vidence from {G}erman administrative data},
  Author                   = {Hethey, Tanja and Schmieder, Johannes F.},
  Institution              = {Institute for Employment Research, Nuremberg, Germany},
  Year                     = {2010},
  Month                    = Aug,
  Number                   = {201006\_en},
  Type                     = {FDZ Methodenreport},
  Abstract                 = {\&quot;Economists have long been interested in the determinants and components of job creation and destruction. In many countries administrative datasets provide an excellent source for detailed analysis on a fine and disaggregate level. However, administrative datasets are not without problems: restructuring and relabeling of firms is often poorly measured and can potentially create large biases. We provide evidence of the extent of this bias and provide a new solution to deal with it using the German Establishment History Panel (BHP). While previous research has relied on the first and last appearance of the establishment identifier (EID) to identify openings and closings, we improve on this approach using a new dataset containing all worker flows between establishments in Germany. This allows us to credibly identify establishment births and deaths from 1975 to 2004. We show that the misclassification bias of using only the EID is very severe: Only about 35 to 40 percent of new and disappearing EIDs with more than 3 employees correspond unambiguously to real establishment entries and exits. Among larger establishments misclassification is even more common. We show that many new establishment IDs appear to be 'Spin-Offs' and these have become increasingly more common over time. We then demonstrate that using only EID entries and exits may dramatically overstate, by as much as 100 percent, the role of establishment turnover for job creation and destruction. Furthermore correcting job creation and destruction measures for spurious EID entries and exits reduces these measures and aligns them closer with the business cycle.\&quot; (Author's abstract, IAB-Doku) ((en))},
  Keywords                 = {labour turnover; },
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12},
  Url                      = {http://ideas.repec.org/p/iab/iabfme/201006_en.html}
}

@Article{HolanEtAl2010,
  Title                    = {Bayesian Multiscale Multiple Imputation With Implications for Data Confidentiality},
  Author                   = {Holan, Scott H. and Toth, Daniell and Ferreira, Marco A. R. and Karr, Alan F.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2010},
  Number                   = {490},
  Pages                    = {564-577},
  Volume                   = {105},
  Abstract                 = { Many scientific, sociological, and economic applications present data that are collected on multiple scales of resolution. One particular form of multiscale data arises when data are aggregated across different scales both longitudinally and by economic sector. Frequently, such datasets experience missing observations in a manner that they can be accurately imputed, while respecting the constraints imposed by the multiscale nature of the data, using the method we propose known as Bayesian multiscale multiple imputation. Our approach couples dynamic linear models with a novel imputation step based on singular normal distribution theory. Although our method is of independent interest, one important implication of such methodology is its potential effect on confidential databases protected by means of cell suppression. In order to demonstrate the proposed methodology and to assess the effectiveness of disclosure practices in longitudinal databases, we conduct a large-scale empirical study using the U.S. Bureau of Labor Statistics Quarterly Census of Employment and Wages (QCEW). During the course of our empirical investigation it is determined that several of the predicted cells are within 1% accuracy, thus causing potential concerns for data confidentiality. },
  Doi                      = {10.1198/jasa.2009.ap08629},
  Eprint                   = {http://dx.doi.org/10.1198/jasa.2009.ap08629},
  Url                      = {http://dx.doi.org/10.1198/jasa.2009.ap08629}
}

@Article{RePEc:bin:bpeajo:v:43:y:2011:i:2011-02:p:73-142,
  Title                    = {{What do Small Businesses Do?}},
  Author                   = {Erik Hurst and Benjamin Wild Pugsley},
  Journal                  = {Brookings Papers on Economic Activity},
  Year                     = {2011},
  Number                   = {2 (Fall)},
  Pages                    = {73-142},
  Volume                   = {43},
  Abstract                 = {In this paper, we show that most small business owners are very different from the entrepreneurs that economic models and policy makers often have in mind. Using new data that samples early stage entrepreneurs just prior to business start up, we show that few small businesses intend to bring a new idea to market. Instead, most intend to provide an existing service to an existing market. Further, we find that most small businesses have little desire to grow big or to innovate in any observable way. We show that such behavior is consistent with the industry characteristics of the majority of small businesses, which are concentrated among skilled craftsmen, lawyers, real estate agents, doctors, small shopkeepers, and restaurateurs. Lastly, we show non pecuniary benefits (being one's own boss, having flexibility of hours, etc.) play a first-order role in the business formation decision. We then discuss how our findings suggest that the importance of entrepreneurial talent, entrepreneurial luck, and financial frictions in explaining the firm size distribution may be overstated. We conclude by discussing the potential policy implications of our findings.},
  Keywords                 = {small business; behavior; policy implications},
  Url                      = {http://ideas.repec.org/a/bin/bpeajo/v43y2011i2011-02p73-142.html}
}

@TechReport{J2j-long,
  Title                    = {Job-to-{J}ob {F}lows: {N}ew Statistics on Worker Reallocation and Job Turnover},
  Author                   = {Henry Hyatt and Erika McEntarfer and Kevin McKinney and Stephen Tibbets and Lars Vilhuber and Douglas Walton},
  Institution              = {U.S. Census Bureau},
  Year                     = {2015},
  Type                     = {mimeo},
  Url                      = {http://lehd.ces.census.gov/doc/jobtojob_documentation_long.pdf}
}

@TechReport{MirandaJarmin2002,
  Title                    = {The {L}ongitudinal {B}usiness {D}atabase},
  Author                   = {Ron Jarmin and Javier Miranda},
  Institution              = {U.S. Census Bureau, Center for Economic Studies},
  Year                     = {2002},
  Number                   = {CES-WP-02-17},
  Type                     = {Discussion Paper},
  Abstract                 = {The LBD is a research dataset constructed at the Census Bureau's Center for Economic Studies. The LBD is an establishment based file created by linking the annual snapshot files from Census Bureau's Business Register over time. It contains high quality longitudinal establishment linkages. Firm level linkages are currently under development at CES. The LBD contains several basic data items such as firm ownership, location, industry, payroll and employment.},
  File                     = {MirandaJarmin2002.pdf:M/MirandaJarmin2002.pdf:PDF},
  Owner                    = {vilhuber},
  Timestamp                = {2009.09.25}
}

@Article{tas2006,
  Title                    = {A Framework for Evaluating the Utility of Data Altered to Protect Confidentiality},
  Author                   = {A. F. Karr and C. N. Kohnen and A. Oganian and J. P. Reiter and A. P. Sanil},
  Journal                  = {The American Statistician},
  Year                     = {2006},
  Number                   = {3},
  Pages                    = {1-9},
  Volume                   = {60},
  Doi                      = {10.1198/000313006X124640}
}

@TechReport{KinneyEtAl2013,
  Title                    = {{SynLBD}: providing firm characteristics on synthetic establishment data},
  Author                   = {Kinney, S. K. and Reiter, J.},
  Institution              = {World Statistics Conference},
  Year                     = {2013},
  Type                     = {Presentation},
  Owner                    = {vilhuber},
  Timestamp                = {2013.06.16}
}

@TechReport{RePEc:cen:wpaper:14-12,
  Title                    = {Improving The {Synthetic Longitudinal Business Database}},
  Author                   = {Satkartar K. Kinney and Jerome P. Reiter and Javier Miranda},
  Institution              = {Center for Economic Studies, U.S. Census Bureau},
  Year                     = {2014},
  Month                    = Feb,
  Number                   = {14-12},
  Type                     = {Working Papers},
  Abstract                 = {In most countries, national statistical agencies do not release establishment-level business microdata, because doing so represents too large a risk to establishments’ confidentiality. Agencies potentially can manage these risks by releasing synthetic microdata, i.e., individual establishment records simulated from statistical models de- signed to mimic the joint distribution of the underlying observed data. Previously, we used this approach to generate a public-use version—now available for public use—of the U. S. Census Bureau’s Longitudinal Business Database (LBD), a longitudinal cen- sus of establishments dating back to 1976. While the synthetic LBD has proven to be a useful product, we now seek to improve and expand it by using new synthesis models and adding features. This article describes our efforts to create the second generation of the SynLBD, including synthesis procedures that we believe could be replicated in other contexts.},
  Url                      = {http://ideas.repec.org/p/cen/wpaper/14-12.html}
}

@Article{SJIAOSKRM2014,
  Title                    = {Improving The {Synthetic Longitudinal Business Database}},
  Author                   = {Satkartar K. Kinney and Jerome P. Reiter and Javier Miranda},
  Journal                  = {Statistical Journal of the IAOS},
  Year                     = {2014},
  Month                    = May,
  Number                   = {2},
  Pages                    = {129--135},
  Volume                   = {30},
  Abstract                 = {In most countries, national statistical agencies do not release establishment-level business microdata, because doing so represents too large a risk to establishments’ confidentiality. Agencies potentially can manage these risks by releasing synthetic microdata, i.e., individual establishment records simulated from statistical models de- signed to mimic the joint distribution of the underlying observed data. Previously, we used this approach to generate a public-use version—now available for public use—of the U. S. Census Bureau’s Longitudinal Business Database (LBD), a longitudinal cen- sus of establishments dating back to 1976. While the synthetic LBD has proven to be a useful product, we now seek to improve and expand it by using new synthesis models and adding features. This article describes our efforts to create the second generation of the SynLBD, including synthesis procedures that we believe could be replicated in other contexts.}
}

@Article{KinneyEtAl2011,
  Title                    = {Towards Unrestricted Public Use Business Microdata: The {S}ynthetic {L}ongitudinal {B}usiness {D}atabase},
  Author                   = {Satkartar K. Kinney and Jerome P. Reiter and Arnold P. Reznek and Javier Miranda and Ron S. Jarmin and John M. Abowd},
  Journal                  = {International Statistical Review},
  Year                     = {2011},
  Month                    = {December},
  Number                   = {3},
  Pages                    = {362-384},
  Volume                   = {79},
  Abstract                 = {In most countries, national statistical agencies do not release establishment-level business microdata, because doing so represents too large a risk to establishments\' confidentiality. One approach with the potential for overcoming these risks is to release synthetic data; that is, the released establishment data are simulated from statistical models designed to mimic the distributions of the underlying real microdata. In this article, we describe an application of this strategy to create a public use file for the Longitudinal Business Database, an annual economic census of establishments in the United States comprising more than 20 million records dating back to 1976. The U.S. Bureau of the Census and the Internal Revenue Service recently approved the release of these synthetic microdata for public use, making the synthetic Longitudinal Business Database the first-ever business microdata set publicly released in the United States. We describe how we created the synthetic data, evaluated analytical validity, and assessed disclosure risk.},
  File                     = {KinneyEtAl2011.pdf:K/KinneyEtAl2011.pdf:PDF},
  Owner                    = {vilhuber},
  Timestamp                = {2012.05.18},
  Url                      = {http://ideas.repec.org/a/bla/istatr/v79y2011i3p362-384.html}
}

@Article{little93,
  Title                    = {Statistical Analysis of Masked Data},
  Author                   = {Roderick J.A. Little},
  Journal                  = {Journal of Official Statistics},
  Year                     = {1993},
  Number                   = {2},
  Pages                    = {407-426},
  Volume                   = {9},
  Owner                    = {John Abowd},
  Timestamp                = {2008.04.29}
}

@Article{Ashwin2008,
  Title                    = {Privacy: {T}heory meets practice on the map},
  Author                   = {Ashwin Machanavajjhala and Daniel Kifer and John M. Abowd and Johannes Gehrke and Lars Vilhuber},
  Journal                  = {International Conference on Data Engineering (ICDE)},
  Year                     = {2008},
  Owner                    = {vilhuber},
  Timestamp                = {2012.05.21}
}

@Article{Meng1994,
  Title                    = {Multiple-imputation inferences with uncongenial sources of input},
  Author                   = {Xiao-Li Meng},
  Journal                  = {Statistical Sciences},
  Year                     = {1994},
  Number                   = {4},
  Pages                    = {538-573},
  Volume                   = {9}
}

@TechReport{MirandaVilhuber2013,
  Title                    = {Looking back on three years of {S}ynthetic {LBD} {B}eta},
  Author                   = {Miranda, Javier and Vilhuber, Lars},
  Institution              = {World Statistics Conference},
  Year                     = {2013},
  Type                     = {Presentation},
  Owner                    = {vilhuber},
  Timestamp                = {2013.06.16}
}

@Book{CNSTATBusinessDynamics,
  Title                    = {Understanding Business Dynamics: An Integrated Data System for America's Future},
  Author                   = {{Panel on Measuring Business Formation, Dynamics, and Performance:} and John 
 Haltiwanger and Lisa M. Lynch and Christopher Mackie},
  Publisher                = {National Research Council, The National Academies Press},
  Year                     = {2007},
  ISBN                     = {9780309104920},
  Url                      = {http://www.nap.edu/openbook.php?record_id=11844}
}

@TechReport{pugsley2014grown,
  Title                    = {Grown-up business cycles},
  Author                   = {Pugsley, Benjamin W and \c{S}ahin, Ay\c{s}egul},
  Institution              = {FRB of New York},
  Year                     = {2015},
  Number                   = {707, revised Sept 2015},
  Type                     = {Staff Report}
}

@Article{reiter03syntheticInference,
  Title                    = {Multiple imputation for statistical disclosure limitation},
  Author                   = {T.E. Raghunathan and J.P. Reiter and D.B. Rubin},
  Journal                  = {Journal of Official Statistics},
  Year                     = {2003},
  Pages                    = {1--16},
  Volume                   = {19},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{reiter03syntheticInference,
  Title                    = {Multiple imputation for statistical disclosure limitation},
  Author                   = {T.E. Raghunathan and J.P. Reiter and D.B. Rubin},
  Journal                  = {Journal of Official Statistics},
  Year                     = {2003},
  Pages                    = {1--16},
  Volume                   = {19},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{reiter03partiallySyntheticInference,
  Title                    = {Inference for Partially Synthetic, Public Use Microdata Sets},
  Author                   = {J.P. Reiter},
  Journal                  = {Survey Methodology},
  Year                     = {2003},
  Number                   = {2},
  Pages                    = {181--188},
  Volume                   = {29},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{reiter03partiallySyntheticInference,
  Title                    = {Inference for Partially Synthetic, Public Use Microdata Sets},
  Author                   = {J.P. Reiter},
  Journal                  = {Survey Methodology},
  Year                     = {2003},
  Number                   = {2},
  Pages                    = {181--188},
  Volume                   = {29},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{reiterCartMI,
  Title                    = {Using CART to generate partially synthetic public use microdata},
  Author                   = {J.P. Reiter},
  Journal                  = {Journal of Official Statistics},
  Year                     = {2005},
  Pages                    = {441--462},
  Volume                   = {21},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{reiterCartMI,
  Title                    = {Using CART to generate partially synthetic public use microdata},
  Author                   = {J.P. Reiter},
  Journal                  = {Journal of Official Statistics},
  Year                     = {2005},
  Pages                    = {441--462},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12},
  Voluem                   = {21}
}

@Article{reiterDisclosureRisk,
  Title                    = {Estimating risks of identification disclosure for microdata},
  Author                   = {J.P. Reiter},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2005},
  Pages                    = {1103 -- 1113},
  Volume                   = { 100},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{reiterDisclosureRisk,
  Title                    = {Estimating risks of identification disclosure for microdata},
  Author                   = {J.P. Reiter},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2005},
  Pages                    = {1103 -- 1113},
  Volume                   = { 100},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{Reiter2003b,
  Title                    = {Model Diagnostics for Remote Access Regression Servers},
  Author                   = {Reiter, Jerome P.},
  Journal                  = {Statistics and Computing},
  Year                     = {2003},
  Note                     = {10.1023/A:1025623108012},
  Pages                    = {371-380},
  Volume                   = {13},
  Abstract                 = {To protect public-use microdata, one approach is not to allow users access to the microdata. Instead, users submit analyses to a remote computer that reports back basic output from the fitted model, such as coefficients and standard errors. To be most useful, this remote server also should provide some way for users to check the fit of their models, without disclosing actual data values. This paper discusses regression diagnostics for remote servers. The proposal is to release synthetic diagnostics?i.e. simulated values of residuals and dependent and independent variables?constructed to mimic the relationships among the real-data residuals and independent variables. Using simulations, it is shown that the proposed synthetic diagnostics can reveal model inadequacies without substantial increase in the risk of disclosures. This approach also can be used to develop remote server diagnostics for generalized linear models.},
  Affiliation              = {Institute of Statistics and Decision Sciences Duke University Box 90251 Durham NC 27708 USA},
  Doi                      = {10.1023/A:1025623108012},
  File                     = {Reiter2003b.pdf:R/Reiter2003b.pdf:PDF},
  ISSN                     = {0960-3174},
  Issue                    = {4},
  Keyword                  = {Mathematics and Statistics},
  Owner                    = {vilhuber},
  Publisher                = {Springer Netherlands},
  Timestamp                = {2012.10.18},
  Url                      = {http://dx.doi.org/10.1023/A:1025623108012}
}

@Article{Reiter2003b,
  Title                    = {Model Diagnostics for Remote Access Regression Servers},
  Author                   = {Reiter, Jerome P.},
  Journal                  = {Statistics and Computing},
  Year                     = {2003},
  Note                     = {10.1023/A:1025623108012},
  Pages                    = {371-380},
  Volume                   = {13},
  Abstract                 = {To protect public-use microdata, one approach is not to allow users access to the microdata. Instead, users submit analyses to a remote computer that reports back basic output from the fitted model, such as coefficients and standard errors. To be most useful, this remote server also should provide some way for users to check the fit of their models, without disclosing actual data values. This paper discusses regression diagnostics for remote servers. The proposal is to release synthetic diagnostics?i.e. simulated values of residuals and dependent and independent variables?constructed to mimic the relationships among the real-data residuals and independent variables. Using simulations, it is shown that the proposed synthetic diagnostics can reveal model inadequacies without substantial increase in the risk of disclosures. This approach also can be used to develop remote server diagnostics for generalized linear models.},
  Affiliation              = {Institute of Statistics and Decision Sciences Duke University Box 90251 Durham NC 27708 USA},
  Doi                      = {10.1023/A:1025623108012},
  File                     = {Reiter2003b.pdf:R/Reiter2003b.pdf:PDF},
  ISSN                     = {0960-3174},
  Issue                    = {4},
  Keyword                  = {Mathematics and Statistics},
  Owner                    = {vilhuber},
  Publisher                = {Springer Netherlands},
  Timestamp                = {2012.10.18},
  Url                      = {http://dx.doi.org/10.1023/A:1025623108012}
}

@Article{reiter04newApproaches,
  Title                    = {New approaches to data dissemination: A glimpse into the future (?)},
  Author                   = {J. P. Reiter},
  Journal                  = {Chance},
  Year                     = {2004},
  Number                   = {3},
  Pages                    = {12--16},
  Volume                   = {17},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{reiter04newApproaches,
  Title                    = {New approaches to data dissemination: A glimpse into the future (?)},
  Author                   = {J. P. Reiter},
  Journal                  = {Chance},
  Year                     = {2004},
  Number                   = {3},
  Pages                    = {12--16},
  Volume                   = {17},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{Reiter2008,
  Title                    = {Multiple imputation when records used for imputation are not used or disseminated for analysis},
  Author                   = {Reiter, Jerome P.},
  Journal                  = {Biometrika},
  Year                     = {2008},
  Number                   = {4},
  Pages                    = {933-946},
  Volume                   = {95},
  Abstract                 = {When some of the records used to estimate the imputation models in multiple imputation are not used or available for analysis, the usual multiple imputation variance estimator has positive bias. We present an alternative approach that enables unbiased estimation of variances and, hence, calibrated inferences in such contexts. First, using all records, the imputer samples m values of the parameters of the imputation model. Second, for each parameter draw, the imputer simulates the missing values for all records n times. From these mn completed datasets, the imputer can analyse or disseminate the appropriate subset of records. We develop methods for interval estimation and significance testing for this approach. Methods are presented in the context of multiple imputation for measurement error.},
  Doi                      = {10.1093/biomet/asn042},
  Eprint                   = {http://biomet.oxfordjournals.org/content/95/4/933.full.pdf+html},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12},
  Url                      = {http://biomet.oxfordjournals.org/content/95/4/933.abstract}
}

@Article{Reiter2008,
  Title                    = {Multiple imputation when records used for imputation are not used or disseminated for analysis},
  Author                   = {Reiter, Jerome P.},
  Journal                  = {Biometrika},
  Year                     = {2008},
  Number                   = {4},
  Pages                    = {933-946},
  Volume                   = {95},
  Abstract                 = {When some of the records used to estimate the imputation models in multiple imputation are not used or available for analysis, the usual multiple imputation variance estimator has positive bias. We present an alternative approach that enables unbiased estimation of variances and, hence, calibrated inferences in such contexts. First, using all records, the imputer samples m values of the parameters of the imputation model. Second, for each parameter draw, the imputer simulates the missing values for all records n times. From these mn completed datasets, the imputer can analyse or disseminate the appropriate subset of records. We develop methods for interval estimation and significance testing for this approach. Methods are presented in the context of multiple imputation for measurement error.},
  Doi                      = {10.1093/biomet/asn042},
  Eprint                   = {http://biomet.oxfordjournals.org/content/95/4/933.full.pdf+html},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12},
  Url                      = {http://biomet.oxfordjournals.org/content/95/4/933.abstract}
}

@TechReport{RePEc:iab:iabdpa:200720,
  Title                    = {Releasing multiply-imputed synthetic data generated in two stages to protect confidentiality},
  Author                   = {Reiter, Jerome P. and Drechsler, Jörg},
  Institution              = {Institut für Arbeitsmarkt- und Berufsforschung (IAB), Nürnberg [Institute for Employment Research, Nuremberg, Germany]},
  Year                     = {2007},
  Month                    = Jun,
  Number                   = {200720},
  Type                     = {IAB Discussion Paper},
  Abstract                 = {\&quot;To protect the cofidentiality of survey respondents' identities and sensitive attributes, statistical agencies can release data in which cofidential values are replaced with multiple imputations. These are called synthetic data. We propose a two-stage approach to generating synthetic data that enables agencies to release different numbers of imputations for different variables. Generation in two stages can reduce computational burdens, decrease disclosure risk, and increase inferential accuracy relative to generation in one stage. We present methods for obtaining inferences from such data. We describe the application of two stage synthesis to creating a public use file for a German business database.\&quot; (Author's abstract, IAB-Doku) ((en))},
  Keywords                 = {IAB-Betriebspanel; Datenaufbereitung; Datenanonymisierung; Datenschutz; angewandte Statistik; statis},
  Url                      = {http://ideas.repec.org/p/iab/iabdpa/200720.html}
}

@TechReport{RePEc:iab:iabdpa:200720,
  Title                    = {Releasing multiply-imputed synthetic data generated in two stages to protect confidentiality},
  Author                   = {Reiter, Jerome P. and Drechsler, Jörg},
  Institution              = {Institut für Arbeitsmarkt- und Berufsforschung (IAB), Nürnberg [Institute for Employment Research, Nuremberg, Germany]},
  Year                     = {2007},
  Month                    = Jun,
  Number                   = {200720},
  Type                     = {IAB Discussion Paper},
  Abstract                 = {\&quot;To protect the cofidentiality of survey respondents' identities and sensitive attributes, statistical agencies can release data in which cofidential values are replaced with multiple imputations. These are called synthetic data. We propose a two-stage approach to generating synthetic data that enables agencies to release different numbers of imputations for different variables. Generation in two stages can reduce computational burdens, decrease disclosure risk, and increase inferential accuracy relative to generation in one stage. We present methods for obtaining inferences from such data. We describe the application of two stage synthesis to creating a public use file for a German business database.\&quot; (Author's abstract, IAB-Doku) ((en))},
  Keywords                 = {IAB-Betriebspanel; Datenaufbereitung; Datenanonymisierung; Datenschutz; angewandte Statistik; statis},
  Url                      = {http://ideas.repec.org/p/iab/iabdpa/200720.html}
}

@Article{Reiter_2009,
  Title                    = {Verification servers: Enabling analysts to assess the quality of inferences from public use data},
  Author                   = {Jerome P. Reiter and Anna Oganian and Alan F. Karr},
  Journal                  = {Computational Statistics {\&} Data Analysis},
  Year                     = {2009},
  Month                    = {feb},
  Number                   = {4},
  Pages                    = {1475--1482},
  Volume                   = {53},
  __markedentry            = {[vilhuber:6]},
  Doi                      = {10.1016/j.csda.2008.10.006},
  Owner                    = {vilhuber},
  Publisher                = {Elsevier {BV}},
  Timestamp                = {2015.12.20},
  Url                      = {http://dx.doi.org/10.1016/j.csda.2008.10.006}
}

@Other{Rodriguez2007,
  Title                    = {Synthetic Data Disclosure Control for {A}merican {C}ommunity {S}urvey Group Quarters},
  Author                   = {Rolando Rodr\'iguez},
  Institution              = {Joint Statistical Meetings},
  Type                     = {presentation},
  Year                     = {2007}
}

@TechReport{RePEc:iaw:iawdip:66,
  Title                    = {Remote Access – Eine Welt ohne Mikrodaten??},
  Author                   = {Gerd Ronning and Philipp Bleninger and Jörg Drechsler and Christopher Gürke},
  Institution              = {Institut für Angewandte Wirtschaftsforschung (IAW)},
  Year                     = {2010},
  Month                    = Jun,
  Number                   = {66},
  Type                     = {IAW Discussion Papers},
  Abstract                 = {Use of microdata is severely hampered in many areas of research. This is in particular true for data from statistical offices. One way to circumvent this problem is to anonymize the data such that both confidentiality is guaranteed and informational content of the data is not to much distorted by the anonymization procedure. However many researchers prefer the use of 'original' data. Therefore in recent years remote access/execution ('Fernrechnen') has become quite popular where the original micro data are used in the statistical analysis but are not available to the researchers. Clearly, this alternative takes more time since program files have to be sent to the statistical office. However, the euphoria for this approach has cooled down a bit since it has become apparent that here also problems of condentiality exist. Most obvious is the fact that residuals cannot be provided. See, for example, Gomatam et al. (2005). However, there are very different kinds of 'disclosures' which are discussed in the paper. The paper also draws attention to the use of saturated models which bear the risk of reproducing confidential tabular data. Analysis of variance is the relevant tool in reproducing magnitude tables whereas the corresponding micro-econometric models can be used to reproduce frequency tables: Logit models give the results in case of a nominal variable and Poisson regression is the approach in case of count data.We also shortly discuss possible disclosure risk in the standard multivariate procedures (factor analysis, principal components, cluster analysis and multidimensional scaling). It is clear from the many examples given in the paper that the remote access/execution option will ask for a large amount of statistical expertise in the statistical office in order to check for disclosure risk. Additionally, there will be a tendency not to provide statistical results to the researcher if critical variables such as region or sector are demanded as regressors in the pr},
  Keywords                 = {Minimum wage; regulation; employment; meta-analysis},
  Url                      = {http://ideas.repec.org/p/iaw/iawdip/66.html}
}

@TechReport{RePEc:iaw:iawdip:66,
  Title                    = {Remote Access – Eine Welt ohne Mikrodaten??},
  Author                   = {Gerd Ronning and Philipp Bleninger and Jörg Drechsler and Christopher Gürke},
  Institution              = {Institut für Angewandte Wirtschaftsforschung (IAW)},
  Year                     = {2010},
  Month                    = Jun,
  Number                   = {66},
  Type                     = {IAW Discussion Papers},
  Abstract                 = {Use of microdata is severely hampered in many areas of research. This is in particular true for data from statistical offices. One way to circumvent this problem is to anonymize the data such that both confidentiality is guaranteed and informational content of the data is not to much distorted by the anonymization procedure. However many researchers prefer the use of 'original' data. Therefore in recent years remote access/execution ('Fernrechnen') has become quite popular where the original micro data are used in the statistical analysis but are not available to the researchers. Clearly, this alternative takes more time since program files have to be sent to the statistical office. However, the euphoria for this approach has cooled down a bit since it has become apparent that here also problems of condentiality exist. Most obvious is the fact that residuals cannot be provided. See, for example, Gomatam et al. (2005). However, there are very different kinds of 'disclosures' which are discussed in the paper. The paper also draws attention to the use of saturated models which bear the risk of reproducing confidential tabular data. Analysis of variance is the relevant tool in reproducing magnitude tables whereas the corresponding micro-econometric models can be used to reproduce frequency tables: Logit models give the results in case of a nominal variable and Poisson regression is the approach in case of count data.We also shortly discuss possible disclosure risk in the standard multivariate procedures (factor analysis, principal components, cluster analysis and multidimensional scaling). It is clear from the many examples given in the paper that the remote access/execution option will ask for a large amount of statistical expertise in the statistical office in order to check for disclosure risk. Additionally, there will be a tendency not to provide statistical results to the researcher if critical variables such as region or sector are demanded as regressors in the pr},
  Keywords                 = {Minimum wage; regulation; employment; meta-analysis},
  Url                      = {http://ideas.repec.org/p/iaw/iawdip/66.html}
}

@Article{rubin93,
  Title                    = {Discussion of Statistical Disclosure Limitation},
  Author                   = {Donald B. Rubin},
  Journal                  = {Journal of Official Statistics},
  Year                     = {1993},
  Number                   = {2},
  Pages                    = {461-468},
  Volume                   = {9},
  Owner                    = {John Abowd},
  Timestamp                = {2008.04.29}
}

@TechReport{RePEc:cen:wpaper:13-19,
  Title                    = {{Synthetic Data For Small Area Estimation In The American Community Survey}},
  Author                   = {Joseph W. Sakshaug and Trivellore E. Raghunathan},
  Institution              = {Center for Economic Studies, U.S. Census Bureau},
  Year                     = {2013},
  Month                    = Apr,
  Number                   = {13-19},
  Type                     = {Working Papers},
  Abstract                 = {Small area estimates provide a critical source of information used to study local populations. Statistical agencies regularly collect data from small areas but are prevented from releasing detailed geographical identifiers in public-use data sets due to disclosure concerns. Alternative data dissemination methods used in practice include releasing summary/aggregate tables, suppressing detailed geographic information in public-use data sets, and accessing restricted data via Research Data Centers. This research examines an alternative method for disseminating microdata that contains more geographical details than are currently being released in public-use data files. Specifically, the method replaces the observed survey values with imputed, or synthetic, values simulated from a hierarchical Bayesian model. Confidentiality protection is enhanced because no actual values are released. The method is demonstrated using restricted data from the 2005-2009 American Community Survey. The analytic validity of the synthetic data is assessed by comparing small area estimates obtained from the synthetic data with those obtained from the observed data.},
  Url                      = {http://ideas.repec.org/p/cen/wpaper/13-19.html}
}

@TechReport{SynLBD20,
  Title                    = {Synthetic {LBD} {Beta} Version 2.0},
  Author                   = {{U.S. Census Bureau}},
  Institution              = {{U.S. Census Bureau} and Cornell University, Synthetic Data Server [distributor]},
  Year                     = {2011},
  Address                  = {Washington,DC and Ithaca, NY, USA},
  Type                     = {[Computer file]},
  Abstract                 = {The Synthetic LBD Beta Data Product (SynLBD) is an experimental data product produced by the U.S. Census Bureau in collaboration with Duke University, Cornell University, the National Institute of Statistical Sciences (NISS), the Internal Revenue Service (IRS) and the National Science Foundation (NSF). The purpose of the SynLBD is to provide users with access to a longitudinal business data product that can be used outside of a secure Census Bureau facility. The Census Bureau created version 2 of the SynLBD by synthesizing information on establishments' employment and payroll, establishments' birth and death years, and industrial classification. The Census Disclosure Review Board and their counterparts at IRS have reviewed the content of the file, and allowed the release of these data for public use.},
  HowPublished             = {Computer file},
  Organization             = {Cornell University, Synthetic Data Server [distributor]},
  Owner                    = {vilhuber},
  Timestamp                = {2013.06.10},
  Url                      = {http://www2.vrdc.cornell.edu/news/data/lbd-synthetic-data/}
}

@TechReport{SynLBD20,
  Title                    = {Synthetic {LBD} {Beta} Version 2.0},
  Author                   = {{U.S. Census Bureau}},
  Institution              = {{U.S. Census Bureau} and Cornell University, Synthetic Data Server
[distributor]},
  Year                     = {2011},
  Address                  = {Washington,DC and Ithaca, NY, USA},
  Type                     = {[Computer file]},
  Abstract                 = {The Synthetic LBD Beta Data Product (SynLBD) is an experimental data
product produced by the U.S. Census Bureau in collaboration with
Duke University, Cornell University, the National Institute of Statistical
Sciences (NISS), the Internal Revenue Service (IRS) and the National
Science Foundation (NSF). The purpose of the SynLBD is to provide
users with access to a longitudinal business data product that can
be used outside of a secure Census Bureau facility. The Census Bureau
created version 2 of the SynLBD by synthesizing information on establishments'
employment and payroll, establishments' birth and death years, and
industrial classification. The Census Disclosure Review Board and
their counterparts at IRS have reviewed the content of the file,
and allowed the release of these data for public use.},
  HowPublished             = {Computer file},
  Organization             = {Cornell University, Synthetic Data Server [distributor]},
  Owner                    = {vilhuber},
  Timestamp                = {2013.06.10},
  Url                      = {http://www2.vrdc.cornell.edu/news/data/lbd-synthetic-data/}
}

@TechReport{LBD2012,
  Title                    = {Longitudinal Business Database ({LBD})},
  Author                   = {{U.S. Census Bureau}},
  Institution              = {{U.S. Census Bureau} [distributor]},
  Year                     = {2012},
  Address                  = {Washington, DC USA},
  Type                     = {[Computer file]},
  Abstract                 = {The Longitudinal Business Database (LBD) has become one of the most frequently requested datasets in the secure Census Research Data Centers (RDCs). By itself or linked to other Census products the LBD provides important new insights about business formation and growth, the nature of competition, labor market dynamics, the nature of business cycles, the sources of productivity growth and the connections to credit markets and financing to name a few. The LBD and Business Dynamic Statistics (BDS) are unique among U.S. longitudinal business databases in providing consistent measures of economic activity at both the establishment and the firm level over a long period of time.
 
 The LBD is a census of business establishments and firms in the U.S. with paid employees comprised of survey and administrative records. The LBD covers all industries and all U.S. States. Access to the LBD is available through the Census Bureau's RDCs},
  HowPublished             = {Computer file},
  Organization             = {{U.S. Census Bureau} [distributor]},
  Owner                    = {vilhuber},
  Timestamp                = {2013.06.10},
  Url                      = {https://www.census.gov/ces/dataproducts/datasets/lbd.html}
}

@TechReport{BDS2012,
  Title                    = {{B}usiness {D}ynamics {S}tatistics 2012 release},
  Author                   = {{U.S. Census Bureau}},
  Institution              = {{U.S. Census Bureau} [distributor]},
  Year                     = {2014},
  Address                  = {Washington,DC},
  Type                     = {[Computer file]},
  HowPublished             = {Computer file},
  Organization             = {{U.S. Census Bureau} [distributor]},
  Owner                    = {vilhuber},
  Timestamp                = {2013.06.10},
  Url                      = {http://www.census.gov/ces/dataproducts/bds/data.html}
}

@TechReport{Vilhuber2013,
  Title                    = {Methods for Protecting the Confidentiality of Firm-Level Data: {I}ssues and Solutions},
  Author                   = {Lars Vilhuber},
  Institution              = {Labor Dynamics Institute},
  Year                     = {2013},
  Month                    = {March},
  Number                   = {19},
  Type                     = {Document},
  Abstract                 = {This report will provide an overview of methods used by statistical agencies to encourage, support, and enhance research access to data for the purpose of generating new knowledge. Quite a few reports and scientific articles have addressed the issue before, and we will be highly indebted to that literature. To a summary of that literature, we hope to provide some recent developments and experiences derived from a decade of working with systems that increase access as both researchers as well as data providers. The report will focus on the data provided by statistical agencies, but it should be understood that government agencies other than a National Statistical Office (NSO) may acquire that function. While excluding the legal background limiting or permitting such data collection and provision, we will highlight some alternate sources and methods, prior to concluding.},
  Owner                    = {vilhuber},
  Timestamp                = {2013.09.20},
  Url                      = {http://digitalcommons.ilr.cornell.edu/ldi/19/}
}

@TechReport{Winkler2007,
  Title                    = {Examples of Easy-to-implement, Widely Used Methods of Masking for which Analytic Properties are not Justified.},
  Author                   = {Winkler, William E.},
  Institution              = {U.S. Census Bureau},
  Year                     = {2007},
  Number                   = {2007-21},
  Type                     = {Research Report Series},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@TechReport{Winkler2007,
  Title                    = {Examples of Easy-to-implement, Widely Used Methods of Masking for which Analytic Properties are not Justified.},
  Author                   = {Winkler, William E.},
  Institution              = {U.S. Census Bureau},
  Year                     = {2007},
  Number                   = {2007-21},
  Type                     = {Research Report Series},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@InProceedings{zayatzjsm2009,
  Title                    = {Disclosure Avoidance for {C}ensus 2010 and {A}merican {C}ommunity {S}urvey Five-year Tabular 
 Data Products},
  Author                   = {Zayatz, Laura and Lucero, Jason and Massell, Paul and Ramanayake, Asoka},
  Booktitle                = {Proceedings of the Joint Statistical Meetings},
  Year                     = {2009},
  Organization             = {American Statistical Association},
  Location                 = {Vancouver, British Columbia, Canada},
  Url                      = {http://www.amstat.org/sections/srms/proceedings/y2010/Files/307156_57962.pdf},
  Urldate                  = {2015-07-15}
}

@Proceedings{DBLP:conf/psd/2012,
  Title                    = {Privacy in Statistical Databases - UNESCO Chair in Data Privacy, International Conference, PSD 2012, Palermo, Italy, September 26-28, 2012. Proceedings},
  Year                     = {2012},
  Editor                   = {Josep Domingo-Ferrer and Ilenia Tinnirello},
  Publisher                = {Springer},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {7556},
  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Booktitle                = {Privacy in Statistical Databases},
  Doi                      = {10.1007/978-3-642-33627-0},
  ISBN                     = {978-3-642-33626-3},
  Owner                    = {vilhuber},
  Timestamp                = {2012.09.26},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-33627-0}
}

@Book{DunneJensenRoberts2009,
  Title                    = {Producer Dynamics: New Evidence from Micro Data},
  Editor                   = {Timothy Dunne and J. Brad Jensen and Mark J. Roberts},
  Publisher                = {The University of Chicago Press for the National Bureau of Economic Research},
  Year                     = {2009},
  Owner                    = {vilhuber},
  Timestamp                = {2006.03.07}
}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_review:}

@Proceedings{DBLP:conf/psd/2004,
	Title                    = {Privacy in Statistical Databases - UNESCO Chair in Data Privacy, International Conference, PSD 2004, Barcelona, Spain, June 9-11, 2004. Proceedings},
	Year                     = {2004},
	Editor                   = {Josep Domingo-Ferrer and Vicenc Torra},
	Publisher                = {Springer},
	Series                   = {Lecture Notes in Computer Science},
	Volume                   = {3050},
	Booktitle                = {Privacy in Statistical Databases},
	Doi                      = {10.1007/978-3-540-22118-0},
	ISBN                     = {978-3-540-22118-0},
	Owner                    = {vilhuber},
	Timestamp                = {2012.09.26},
	Url                      = {http://dx.doi.org/10.1007/978-3-540-22118-0}
}

@InCollection{AbowdLane2004,
	title = {New Approaches to Confidentiality Protection Synthetic Data, Remote	Access and Research Data Centers},
	author = {John M. Abowd and Julia I. Lane},
	Booktitle = {Privacy in Statistical Databases},
	Year = {2004},
	Pages = {282-289},
	Crossref = {DBLP:conf/psd/2004},
	Keywords = {Data Archive; Data Curation; Statistical Disclosure Limitation; Privacy-preserving Datamining},
	Owner = {vilhuber},
	URL = {http://www.springer.com/la/book/9783540221180}
}

@ARTICLE{2014arXiv1412.2282H,
	author = {{Hu}, J. and {Reiter}, J.~P. and {Wang}, Q.},
	title = "{Dirichlet Process Mixture Models for Nested Categorical Data}",
	journal = {ArXiv e-prints},
	archivePrefix = "arXiv",
	eprint = {1412.2282},
	primaryClass = "stat.ME",
	keywords = {Statistics - Methodology},
	year = 2014,
	month = dec,
	adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1412.2282H},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@TechReport{RePEc:nbr:nberwo:16300,
	author={John C. Haltiwanger and Ron S. Jarmin and Javier Miranda},
	title={{Who Creates Jobs? Small vs. Large vs. Young}},
	year=2010,
	month=Aug,
	institution={National Bureau of Economic Research, Inc},
	type={NBER Working Papers},
	url={https://ideas.repec.org/p/nbr/nberwo/16300.html},
	number={16300},
	abstract={The view that small businesses create the most jobs remains appealing to policymakers and small business advocates. Using data from the Census Bureau Business Dynamics Statistics and Longitudinal Business Database, we explore the many issues at the core of this ongoing debate. We find that the relationship between firm size and employment growth is sensitive to these issues. However, our main finding is that once we control for firm age there is no systematic relationship between firm size and growth. Our findings highlight the important role of business startups and young businesses in U.S. job creation.},
	keywords={},
	doi={},
}